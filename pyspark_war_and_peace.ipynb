{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP30CaiszMhtmnODrycvq5H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomouvic/CSC502/blob/main/pyspark_war_and_peace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of the War and Peace novel"
      ],
      "metadata": {
        "id": "7yNLTss536NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To be run on Google Colab**.\n",
        "\n",
        "**The following cell can take long, about 3 min**. Only execute it once per session.  "
      ],
      "metadata": {
        "id": "2fozSePiEONW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7CqdE1VhI8_"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop2.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop2.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init(\"spark-3.3.2-bin-hadoop2\")# SPARK_HOME\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "EHiNKaYgC0pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usmMrr5Uhipf"
      },
      "source": [
        "**Create an RDD from a text file**\n",
        "\n",
        "Each line of the text file becomes an element of the RDD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVfPs26GjAal"
      },
      "source": [
        "!wget http://www.gutenberg.org/files/2600/2600-0.txt -O war_and_peace.txt\n",
        "textFile = sc.textFile('war_and_peace.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kktqT8BX_H9H"
      },
      "source": [
        "#One common transformation is\n",
        "#filtering data that matches a predicate.\n",
        "#We can use this to create a new RDD\n",
        "#holding just the strings that contain\n",
        "#the word Anna.\n",
        "\n",
        "# The filter() transformation returns a new RDD\n",
        "# containing only the elements that satisfy a predicate.\n",
        "# A predicate is a function that returns True or False\n",
        "# given an element of the RDD.\n",
        "# The following function \"lambda x: \"Anna\" in x\",\n",
        "# given an element x of the RDD, a line in this case,\n",
        "# returns condition '\"Anna\" in x', which can be True or False.\n",
        "annaLines = textFile.filter(lambda x: \"Anna\" in x)\n",
        "\n",
        "#One example of an action is first()\n",
        "#which returns the first element in an RDD.\n",
        "firstLine = annaLines.first()\n",
        "\n",
        "print(firstLine)\n",
        "\n",
        "#Another example of action is collecting\n",
        "#all the elements of an RDD.\n",
        "allAnnaLines = annaLines.collect()\n",
        "\n",
        "print(allAnnaLines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zsCzlk6ywOa"
      },
      "source": [
        "#map() takes in a function and applies it to each element in the RDD\n",
        "#with the result of the function being the new value of each element\n",
        "#in the resulting RDD.\n",
        "\n",
        "rdd = sc.parallelize([1, 2, 3, 4]);\n",
        "result = rdd.map(lambda x: x*x);\n",
        "print(result.collect());"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cHbdtMFHhPb"
      },
      "source": [
        "#Sometimes we want to produce multiple output elements for each input element.\n",
        "#The operation to do this is called flatMap().\n",
        "#As with map(), the function we provide to flatMap() is called individually\n",
        "#for each element in the input RDD.\n",
        "#Instead of returning a single element, we return in this function an iterator\n",
        "#with our return values.\n",
        "#Rather than producing an RDD of iterators, flatMap() gives back an RDD\n",
        "#of the elements from all of the iterators.\n",
        "\n",
        "#A simple usage of flatMap() is splitting up an input string into words.\n",
        "#From each line, we want to output multiple words.\n",
        "\n",
        "words = textFile.flatMap(lambda x: x.split());\n",
        "\n",
        "print(words.collect()[0:100])\n",
        "print(words.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekVNs4R630BR"
      },
      "source": [
        "#Suppose we would like to transform our string RDD of words\n",
        "#to an RDD of the word lengths so that we can compute different stats with ease.\n",
        "\n",
        "wordLength = words.map(lambda x: len(x));\n",
        "\n",
        "#Then, we can compute different stats on it. E.g.\n",
        "\n",
        "wordAvgLength = wordLength.mean();\n",
        "\n",
        "print(wordAvgLength)\n",
        "\n",
        "#and quite a few others (min, max, stdev, histograms, etc).\n",
        "print(wordLength.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plVsP3Ct5pyF"
      },
      "source": [
        "#The most common action on basic RDDs you will likely use is reduce(),\n",
        "#which takes a function that operates on two elements of the type in your RDD\n",
        "#and returns a new element of the same type.\n",
        "\n",
        "#A simple example of such a function is +, which we can use to sum our RDD.\n",
        "#With reduce(), we can easily sum the elements of our RDD,\n",
        "#count the number of elements, and perform other types of aggregations.\n",
        "\n",
        "rdd = sc.parallelize([1, 2, 3, 4]);\n",
        "sum = rdd.reduce(lambda x,y: x+y);\n",
        "print(sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8gAgwwS6bSR"
      },
      "source": [
        "#reduce() requires that the return type of our result be the same type as that\n",
        "#of the elements in the RDD we are operating over.\n",
        "#This works well for operations like sum,\n",
        "#but sometimes we want to return a different type.\n",
        "\n",
        "#For example, when computing a running average,\n",
        "#we need to keep track of both the count so far and the number of elements,\n",
        "#which requires us to return a pair.\n",
        "#We could work around this by first using map() where we transform every element\n",
        "#into the element and the number 1, which is the type we want to return,\n",
        "#so that the reduce() function can work on pairs.\n",
        "\n",
        "rdd = sc.parallelize([1, 2, 3, 4])\n",
        "sumcnt = rdd.map(lambda x: (x,1) ).reduce(lambda t,r: (t[0]+r[0], t[1]+r[1]) )\n",
        "\n",
        "avg = sumcnt[0] / sumcnt[1]\n",
        "print(avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgCSJ-CmGMS2"
      },
      "source": [
        "#The aggregate() action frees us from the constraint of having the return\n",
        "#be the same type as the RDD we are working on.\n",
        "#With aggregate(), we supply:\n",
        "#(1) An initial “zero” value of the type we want to return.\n",
        "#(2) A function to combine the elements from our RDD with the “accumulator”.\n",
        "#(3) A second function to “merge” two accumulators,\n",
        "#    given that each machine accumulates its own results locally.\n",
        "\n",
        "#We can use aggregate() to compute the average of an RDD,\n",
        "#avoiding a map() before the reduce().\n",
        "\n",
        "rdd = sc.parallelize([1, 2, 3, 4])\n",
        "sumcnt = rdd.aggregate((0, 0),\n",
        "                       lambda acc, value: (acc[0] + value, acc[1] + 1),\n",
        "                       lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\t)\n",
        "avg = sumcnt[0] / sumcnt[1]\n",
        "print(avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4X1nFfPJuhJ"
      },
      "source": [
        "**RDDs of key/value pairs**\n",
        "\n",
        "Spark provides operations on RDDs containing key/value pairs.\n",
        "These RDDs are called pair RDDs. Pair RDDs allow you to act on each key in parallel.\n",
        "For example, pair RDDs have a reduceByKey() method (analogous to reduce for regular RDDs) that can aggregate data separately for each key.\n",
        "We can create pair RDDs from existing RDDs. E.g.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKmm3v6YH6W5"
      },
      "source": [
        "import re\n",
        "words = textFile.flatMap(lambda x: re.findall('\\w+', x));\n",
        "\n",
        "lw = words.map( lambda x: (len(x), x) );\n",
        "\n",
        "# This creates an RDD of length-word pairs.\n",
        "# What can we do with it?\n",
        "# We can find for example the number of words for each length.\n",
        "\n",
        "r = lw.countByKey();\n",
        "print(r)\n",
        "\n",
        "# Or, we can collect all the words of length >= 16.\n",
        "\n",
        "longwordsRDD = lw.groupByKey().filter(lambda x: x[0] >= 16)\n",
        "\n",
        "print(longwordsRDD.collect())\n",
        "\n",
        "#What we get back is an object which allows iterating over the results.\n",
        "#Turn the results of groupByKey into a list by calling list() on the values, e.g.\n",
        "\n",
        "print(longwordsRDD.map(lambda x : (x[0], list(x[1]))).collect())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjzNNELJbC_Y"
      },
      "source": [
        "**Word count**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KxEJ33sbGpG"
      },
      "source": [
        "textFile = sc.textFile('war_and_peace.txt')\n",
        "\n",
        "word_counts = textFile.flatMap(lambda x: x.split()) \\\n",
        "                      .map(lambda word: (word,1)) \\\n",
        "                      .reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "print(word_counts.collect())\n",
        "\n",
        "# Those familiar with the combiner concept from MapReduce should note that\n",
        "# calling reduceByKey() will automatically perform combining locally\n",
        "# on each machine before computing global totals for each key.\n",
        "# The user does not need to specify a combiner."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjm27-Tye-Yo"
      },
      "source": [
        "**Word count with stopwords removed**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-1TLn0De8Tq"
      },
      "source": [
        "!wget \"https://gist.githubusercontent.com/sebleier/554280/raw/7e0e4a1ce04c2bb7bd41089c9821dbcf6d0c786c/NLTK's%2520list%2520of%2520english%2520stopwords\" -O stopwords.txt\n",
        "\n",
        "textFile = sc.textFile('war_and_peace.txt')\n",
        "stopwords = sc.textFile('stopwords.txt')\n",
        "\n",
        "word_counts = textFile.flatMap(lambda x: x.split()) \\\n",
        "                      .map(lambda word: (word.lower(),1)) \\\n",
        "                      .subtractByKey(stopwords.map(lambda word: (word, 1))) \\\n",
        "                      .reduceByKey(lambda a,b: a+b)\n",
        "\n",
        "print(word_counts.collect())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}